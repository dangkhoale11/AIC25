{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Search Data Preparation Pipeline\n",
    "\n",
    "This notebook prepares your OCR and object detection data for a hybrid search system. It produces two main outputs:\n",
    "\n",
    "1.  `keyword_data.csv`: A file ready for a keyword/lexical search engine like **Elasticsearch** or a **BM25** index.\n",
    "2.  `vector_data.pkl`: A file containing vector embeddings, ready for a vector database like **Milvus**.\n",
    "\n",
    "You can run this notebook in an environment like Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Raw Data\n",
    "\n",
    "We'll load the data from the `ocr_data.jsonl` file. This file should be uploaded to your Colab environment alongside this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "with open('ocr_data.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data)} records.\")\n",
    "print(\"First record:\", data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Process Data and Generate Embeddings\n",
    "\n",
    "Now, we'll iterate through the data and prepare it for our two different search indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "print(\"Loading sentence transformer model... (This may take a moment)\")\n",
    "# Using a multilingual model as the sample data contains Vietnamese\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "keyword_data = []\n",
    "vector_data = []\n",
    "\n",
    "for item in tqdm(data, desc=\"Processing records\"):\n",
    "    keyframe_id = item['keyframe_id']\n",
    "    ocr_text = \" \".join(item['ocr_text'])\n",
    "    object_text = \" \".join(item['object'])\n",
    "    \n",
    "    # 1. Prepare data for KEYWORD search (BM25/Elasticsearch)\n",
    "    # We create a single, simple text field containing all keywords.\n",
    "    combined_text_for_keyword = f\"{ocr_text} {object_text}\"\n",
    "    keyword_data.append({\n",
    "        'keyframe_id': keyframe_id,\n",
    "        'text': combined_text_for_keyword\n",
    "    })\n",
    "    \n",
    "    # 2. Prepare data for VECTOR search (Milvus)\n",
    "    # We create a more descriptive string for better semantic embedding.\n",
    "    combined_text_for_vector = f\"OCR text: {ocr_text}. Detected objects: {object_text}.\"\n",
    "    embedding = model.encode(combined_text_for_vector, convert_to_tensor=False).tolist()\n",
    "    vector_data.append({\n",
    "        'keyframe_id': keyframe_id,\n",
    "        'embedding': embedding\n",
    "    })\n",
    "\n",
    "print(\"Processing complete.\")\n",
    "print(f\"{len(keyword_data)} records prepared for keyword search.\")\n",
    "print(f\"{len(vector_data)} records prepared for vector search.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Save Processed Data for Migration\n",
    "\n",
    "Now we save the processed data into two separate files. These files are what you will 'migrate' to your production search systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data for keyword search engine\n",
    "keyword_df = pd.DataFrame(keyword_data)\n",
    "keyword_df.to_csv('keyword_data.csv', index=False)\n",
    "print(\"Saved keyword data to keyword_data.csv\")\n",
    "\n",
    "# Save data for vector database\n",
    "with open('vector_data.pkl', 'wb') as f:\n",
    "    pickle.dump(vector_data, f)\n",
    "print(\"Saved vector data to vector_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Verify Outputs and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migration and Hybrid Search Guide\n",
    "\n",
    "You now have `keyword_data.csv` and `vector_data.pkl`. Hereâ€™s how to use them:\n",
    "\n",
    "**1. For the Keyword Search Engine (e.g., Elasticsearch):**\n",
    "   - Create an index in Elasticsearch with a mapping for `keyframe_id` and `text`.\n",
    "   - Use the Elasticsearch bulk ingest API or a client library to upload the data from `keyword_data.csv` into this index.\n",
    "\n",
    "**2. For the Vector Database (Milvus):**\n",
    "   - Create a collection in Milvus. Your schema should include a primary key (`keyframe_id`), the vector field (`embedding`), and any other metadata you want to store.\n",
    "   - Write a Python script using `pymilvus` to:\n",
    "     - Load the `vector_data.pkl` file (`pickle.load(open('vector_data.pkl', 'rb'))`).\n",
    "     - Iterate through the list of records and insert them into your Milvus collection in batches.\n",
    "\n",
    "**3. Implementing the Hybrid Search Query:**\n",
    "   - Your application's search function will now query **both** systems with the user's input.\n",
    "   - **Query 1:** Send the query to Elasticsearch to get a list of `keyframe_id`s based on keyword matches (BM25 score).\n",
    "   - **Query 2:** Embed the user's query using the same sentence transformer model and send it to Milvus to get a list of `keyframe_id`s based on vector similarity (e.g., cosine similarity or L2 distance).\n",
    "   - **Fuse the Results:** Combine the two lists of results. A common technique is **Reciprocal Rank Fusion (RRF)**, where you give each result a score based on its rank in its respective list, sum the scores for items that appear in both lists, and then sort to get a final, blended ranking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
